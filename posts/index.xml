<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Haiwen</title>
    <link>https://haiwenzhang.github.io/posts/</link>
    <description>Recent content in Posts on Haiwen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Nov 2018 10:10:49 +0000</lastBuildDate>
    
	<atom:link href="https://haiwenzhang.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>设计模式之简单工厂模式</title>
      <link>https://haiwenzhang.github.io/posts/design-pattern-simple-factory/</link>
      <pubDate>Thu, 01 Nov 2018 10:10:49 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/design-pattern-simple-factory/</guid>
      <description>&lt;h2 id=&#34;一-定义&#34;&gt;一、定义&lt;/h2&gt;

&lt;p&gt;简单工厂模式(Simple Factory Pattern)：又称为静态工厂方法(Static Factory Method)模式，它属于类创建型模式。在简单工厂模式中，可以根据参数的不同返回不同类的实例。简单工厂模式专门定义一个类来负责创建其他类的实例，被创建的实例通常都具有共同的父类。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>机器学习基石笔记：学习的类型</title>
      <link>https://haiwenzhang.github.io/posts/ML-Foundations-Notes03/</link>
      <pubDate>Sun, 08 Jul 2018 19:50:45 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/ML-Foundations-Notes03/</guid>
      <description>&lt;h3 id=&#34;一-learning-with-different-output-space&#34;&gt;一、 Learning with Different Output Space&lt;/h3&gt;

&lt;p&gt;二元分类问题：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
y=\{+1, -1\}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;输出只有两个，+1表示正类，-1表示负类。二元分类的问题很常见，例如信用卡发放、垃圾邮件判别等等。二元分类是机器学习最基本的问题。二元分类有线性模型和非线性模型，需要依据具体的问题选择对应的模型。下图展示了常见的二元分类：&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>机器学习基石笔记：感知机</title>
      <link>https://haiwenzhang.github.io/posts/ML-Foundations-Notes02/</link>
      <pubDate>Thu, 14 Jun 2018 16:10:05 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/ML-Foundations-Notes02/</guid>
      <description>&lt;h2 id=&#34;一perceptron-hypothesis-set&#34;&gt;一、Perceptron Hypothesis Set&lt;/h2&gt;

&lt;p&gt;假设:&lt;/p&gt;

&lt;p&gt;特征空间：$x \subseteq R^n$&lt;/p&gt;

&lt;p&gt;输出空间：$y={+1, -1}$&lt;/p&gt;

&lt;p&gt;于是关于感知机有：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
h(x) = sign(\sum_{i=1}^nw_ix_i - threshold)
\]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>机器学习基石笔记：学习问题</title>
      <link>https://haiwenzhang.github.io/posts/ML-Foundations-Notes01/</link>
      <pubDate>Sun, 10 Jun 2018 20:54:45 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/ML-Foundations-Notes01/</guid>
      <description>&lt;h2 id=&#34;一what-is-machine-learning&#34;&gt;一、What is Machine Learning&lt;/h2&gt;

&lt;p&gt;首先解释一下什么是“学习”？对于学习一词我们并不陌生，从来到这个世界的第一天起我们就开始用我们好奇的眼睛和智慧的大脑去观察这个世界、理解世界，在不断的练习和积累之后掌握了某种技能，这便是“学习”。同样了我们希望机器也可以像人类一样，通过观察大量的数据训练，发现事物规律，获得某种分析问题、解决问题的能力，这就是“机器学习”。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Spark入门：可视化RDD的算子(3)</title>
      <link>https://haiwenzhang.github.io/posts/visual-rdd-api-03/</link>
      <pubDate>Fri, 04 May 2018 19:37:39 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/visual-rdd-api-03/</guid>
      <description>&lt;h2 id=&#34;1-spark算子-mappartitions&#34;&gt;1. Spark算子：mapPartitions&lt;/h2&gt;

&lt;p&gt;类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&amp;gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val x = sc.parallelize(Array(1,2,3), 2)
def f(i:Iterator[Int])={ (i.sum,42).productIterator }
val y = x.mapPartitions(f)

// glom() flattens elements on the same partition
val xOut = x.glom().collect()
val yOut = y.glom().collect()

// x: Array(Array(1), Array(2, 3))
// y: Array(Array(1, 42), Array(5, 42))
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Spark入门：可视化RDD的算子(2)</title>
      <link>https://haiwenzhang.github.io/posts/visual-rdd-api-02/</link>
      <pubDate>Thu, 03 May 2018 19:37:39 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/visual-rdd-api-02/</guid>
      <description>&lt;h2 id=&#34;1-spark算子-groupby&#34;&gt;1. Spark算子：groupBy&lt;/h2&gt;

&lt;p&gt;groupBy(f) f返回key，传入的RDD的各个元素根据这个key进行分组。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val x = sc.parallelize(Array(&amp;quot;John&amp;quot;, &amp;quot;Fred&amp;quot;, &amp;quot;Anna&amp;quot;, &amp;quot;James&amp;quot;))
val y = x.groupBy(w =&amp;gt; w.charAt(0))
println(x.collect().mkString(&amp;quot;,&amp;quot;))
println(y.collect().mkString(&amp;quot;,&amp;quot;))

// x: [&amp;quot;John&amp;quot;, &amp;quot;Fred&amp;quot;, &amp;quot;Anna&amp;quot;, &amp;quot;James&amp;quot;]
// y: [(&#39;A&#39;,[&#39;Anna&#39;]),(&#39;J&#39;,[&#39;John&#39;, &#39;James&#39;]),(&#39;F&#39;,[&#39;Fred&#39;])]
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Spark入门：可视化RDD的算子(1)</title>
      <link>https://haiwenzhang.github.io/posts/visual-rdd-api-01/</link>
      <pubDate>Wed, 02 May 2018 13:24:51 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/visual-rdd-api-01/</guid>
      <description>&lt;h2 id=&#34;1-spark算子-map&#34;&gt;1. Spark算子：map&lt;/h2&gt;

&lt;p&gt;map是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD，新RDD叫作MappedRDD(this, sc.clean(f))。任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val x = sc.parallelize(Array(&amp;quot;b&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;))
val y = x.map(z =&amp;gt; (z, 1))
println(x.collect().mkString(&amp;quot;,&amp;quot;))
println(y.collect().mkString(&amp;quot;,&amp;quot;))

// x: [&amp;quot;b&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;]
// y: [(&amp;quot;b&amp;quot;, 1), (&amp;quot;a&amp;quot;, 1), (&amp;quot;c&amp;quot;, 1)]
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Spark入门：核心概念RDD</title>
      <link>https://haiwenzhang.github.io/posts/sparkrdd/</link>
      <pubDate>Tue, 01 May 2018 12:13:53 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/sparkrdd/</guid>
      <description>&lt;h2 id=&#34;1-1-什么是spark&#34;&gt;1.1 什么是Spark？&lt;/h2&gt;

&lt;p&gt;Spark最初由美国加州伯克利大学（UCBerkeley）的AMP（Algorithms, Machines and People）实验室于2009年开发，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程序。Spark在诞生之初属于研究性项目，其诸多核心理念均源自学术研究论文。2013年，Spark加入Apache孵化器项目后，开始获得迅猛的发展，如今已成为Apache软件基金会最重要的三大分布式计算系统开源项目之一（即Hadoop、Spark、Storm）。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HDFS读写浅析</title>
      <link>https://haiwenzhang.github.io/posts/hdfstheory/</link>
      <pubDate>Fri, 27 Apr 2018 14:29:09 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/hdfstheory/</guid>
      <description>&lt;h2 id=&#34;一-hdfs-架构&#34;&gt;一、HDFS 架构&lt;/h2&gt;</description>
    </item>
    
    <item>
      <title>HDFS 简介</title>
      <link>https://haiwenzhang.github.io/posts/hdfsinf/</link>
      <pubDate>Mon, 23 Apr 2018 15:56:22 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/hdfsinf/</guid>
      <description>&lt;h2 id=&#34;1-什么是hdfs&#34;&gt;1 什么是HDFS?&lt;/h2&gt;

&lt;p&gt;HDFS是一个分布式文件系统，适合部署在廉价的机器上。HDFS适合一次写入，多次读取的场景，且不支持文件的修改。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rails Nginx 实现下载受限文件的优化</title>
      <link>https://haiwenzhang.github.io/posts/rails-nginx-sendfile/</link>
      <pubDate>Sat, 24 Feb 2018 20:29:43 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/rails-nginx-sendfile/</guid>
      <description>&lt;h2 id=&#34;需求&#34;&gt;需求&lt;/h2&gt;

&lt;p&gt;在Web开发中经常遇到一些文件需要用户认证和授权才能够访问，实现这样的功能需要在App Server进行。当遇到大文件或者并发量增加的时候，容易造成性能问题。为什么会这样呢？让我们从操作系统层面来分析一下：&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Scala函数式编程札记01</title>
      <link>https://haiwenzhang.github.io/posts/scala-fp-01/</link>
      <pubDate>Mon, 25 Dec 2017 16:48:17 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/scala-fp-01/</guid>
      <description>&lt;p&gt;最近为了研究Spark，开启了Scala的学习之旅，然后便一发不可收拾。。。。。。那么Scala是一门什么样的语言呢？有些人说它是JVM上的C++；也有些人说它是Haskell，不过比Haskell弱。Scala的官网上有一句话：Object-Oriented Meets Functional，什么意思呢？简单来说就是“面向对象”这位帅气的小伙子偶遇了“函数式”这位邻家小妹妹，发生了一段计算机界惊天动地的情缘，于是便产生了Scala。一说到了面向对象，C++、Java、Python等便在脑海里显现出来了，可是对于函数式编程很多工程师肯定是只听其声不见其人。那么今天就随我一起去揭开邻家小妹妹神秘的面纱吧！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ruby元编程读书笔记01</title>
      <link>https://haiwenzhang.github.io/posts/metaprograming-in-ruby-01/</link>
      <pubDate>Sun, 24 Dec 2017 09:42:13 +0000</pubDate>
      
      <guid>https://haiwenzhang.github.io/posts/metaprograming-in-ruby-01/</guid>
      <description>&lt;h2 id=&#34;什么是元编程&#34;&gt;什么是元编程？&lt;/h2&gt;

&lt;p&gt;元编程: 编写能在运行时操作语言构件的代码.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class Movie &amp;lt; ActiveRecord::Base
end

movie= Movie.create
movie.title = &amp;quot;Zootropolis&amp;quot;
move.title
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>